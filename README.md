# Tackling-Challenges-in-Automated-Data-Extraction-Using-Large-Language-Models

Systematic reviews, particularly meta-analyses, involve a crucial yet labor-intensive and error-prone stage of data extraction. Recent advances in large language models (LLMs) have unlocked new avenues for automating this process, potentially enhancing both efficiency and reliability. In a recent study, Jansen et al. (2025) systematically evaluated the accuracy and error patterns of LLM-assisted data extraction across 22 reviews published in Psychological Bulletin. Their findings indicated that while achieving acceptable to good accuracy for some variables describing study characteristics, LLMs struggled with numerical variables, especially those related to effect sizes. In this commentary, we discuss the current challenges for automated data extraction in general and potential pathways for improving the work reported in Jansen et al.'s article. We situate our discussion within the framework of context engineering, which aims to dynamically refine the information provided to LLMs for better performance. We identify five key challenges that reflect either LLMs’ unique “behavioral patterns” or common practices in research synthesis: parsing semi-structured data, understanding long contexts, performing arithmetic induction, engaging in complex reasoning, and ensuring the reproducibility of coding protocols. We then outline potential solutions inspired by context-engineering implementations such as retrieval-augmented generation and tool-integrated reasoning. To illustrate these ideas, we present several examples, including extracting semi-structured data via optical character recognition, reliably computing effect sizes through function calling, and performing adaptive information retrieval with LLM-based agents. We conclude by calling for future research in automated data extraction to advance beyond simple instruction-following paradigms toward more reliable forms of context engineering.
